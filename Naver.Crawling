# -*- coding: utf-8 -*-

#3.10 SSL 강화로 Crawling이 힘듬으로 3.9.6 추천

# 단축키
# 쪼개기: CTRL + SHIFT + -
# 범위주석 : DRAG + CTRL + ?
# 한꺼번에 지정하기: ALT + SHIFT + DRAG
# 변수 한번에 하이라이트: 더블 클릭
# 원하는 방향으로 하이라이트: SHIFT + 방향키
# 시작위치 부터 끝까지 하이라이트: SHIFT + END
# 전체 선택: CTRL + A


import logging
import warnings
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.remote.remote_connection import LOGGER
from datetime import datetime, timedelta
from datetime import datetime
import pytz
import threading
import requests
import pandas as pd
import numpy as np
import re
import os 
from tqdm import tqdm 
import time
import random
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import ssl
from requests.adapters import HTTPAdapter

url = f"https://search.naver.com/search.naver?fbm=0&ie=utf8&sm=top_hty&where=nexearch&query="
req = requests.get(url,headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36',
    })



#timezone설정
timezone = pytz.timezone('Asia/Seoul') # Replace 'Asia/Seoul with the timezone you need
current_time_in_timezone = datetime.now(timezone)

#Warning Log 및 Selenium Headless Mode로 설정
options = webdriver.ChromeOptions()
options.add_argument('--headless')
LOGGER.setLevel(logging.ERROR)
warnings.filterwarnings("ignore", category=InsecureRequestWarning)

#함수 선언
def initialize_variables():
    title_text=[]
    link_text=[]
    source_text=[]
    date_text=[]
    contents_text=[]
    summary_contents_text=[]
    result={}
    return result ,date_text, title_text, source_text,summary_contents_text, contents_text, link_text

def get_basic_info():
    df_company = pd.read_excel('./crawling_files/CompanyName.xlsx')
    df_keyword = pd.read_excel('./crawling_files/keyword.xlsx')
    company_list = list(df_company['CompanyName'])
    keyword_list = list(df_keyword['keyword'])
    return company_list, keyword_list


def date_cleansing(test, date_text):
    try:
        #지난 뉴스
        #머니투데이  10면1단  2018.11.05.  네이버뉴스   보내기  
        pattern = '\d+.(\d+).(\d+).'  #정규표현식 
    
        r = re.compile(pattern)
        match = r.search(test).group(0)  # 2018.11.05.
        date_text.append(match)
    except AttributeError:
        #최근 뉴스
        #이데일리  1시간 전  네이버뉴스   보내기  
        pattern = '\w* (\d\w*)'     #정규표현식 
        
        r = re.compile(pattern)
        match = r.search(test).group(1)
        #print(match)
        date_text.append(match)


#내용 정제화 함수 
def contents_cleansing(contents, contents_text):
    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', #HTML <-
                                      str(contents)).strip()  #앞에 필요없는 부분 제거
    second_cleansing_contents = re.sub('<ul class="relation_lst">.*?</dd>', '', 
                                       first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)
    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()
    contents_text.append(third_cleansing_contents)
    #print(contents_text)

def extract_full_article_content(article_url):
    # headers = {
    #     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
    #     "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    # }
    
    try:
        custom_ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
        custom_ssl_context.set_ciphers("DEFAULT:!DH")#HIGH:!DH:!aNULL
        session = requests.Session()
        session.mount('https://', HTTPAdapter(max_retries=10))  # Optional, add retries if needed

        # Suppress the warning about the weak DH key size
        session.verify = False
        article_response = session.get(article_url, headers=headers)
        # article_response = requests.get(article_url, headers=headers, verify=False, timeout=10, stream=True, cert=False, ssl_context=ssl_context)
        article_soup = BeautifulSoup(article_response.content, 'html.parser')
        content_element = article_soup.find("div", itemprop="articleBody") or \
                          article_soup.find("div", class_=re.compile("content"))
        content = content_element.get_text(strip=True) if content_element else "Content not found or not accessible"
        return content
    except requests.exceptions.RequestException as e:
        print(f"An error occurred while requesting the article URL {article_url}: {e}")
        return 'No Article'

def soup_function(html, date_text, title_text, source_text, contents_text, summary_contents_text, link_text):
    soup = BeautifulSoup(html, 'html.parser')

    #<a>태그에서 제목과 링크주소 추출
    atags = soup.select('.news_tit')
    for atag in atags:
        title_text.append(atag.text)     #제목
        link_text.append(atag['href'])   #링크주소 <a href='링크
    #신문사 추출
    source_lists = soup.select('.info_group > .press')
    
    for source_list in source_lists:
        source_text.append(source_list.text)    #신
    #날짜 추출 
    date_lists = soup.select('.info_group > span.info')

    for date_list in date_lists:
    # 1면 3단 같은 위치 제거
        if date_list.text.find("면") == -1:
            date_text.append(date_list.text)

    #본문요약본
    summary_contents_lists = soup.select('.news_dsc')
    for contents_list in summary_contents_lists:
        contents_cleansing(contents_list, summary_contents_text) #본문요약 정

    # Add code to extract full article content
    atags = soup.select('.news_tit')  # or your specific selector for the article link
    for atag in atags:
        article_url = atag['href']  # Get the article URL
        full_content = extract_full_article_content(article_url)  # Extract the full content
        contents_text.append(full_content)  # Append the full content to contents_text

    #모든 리스트 딕셔너리형태로 저장
    res = {"date" : date_text , "title":title_text ,  "source" : source_text ,"contents_summary": summary_contents_text,"contents_full": contents_text ,"link":link_text } 
    return res

def crawl_companies_chunk(companies, maxpage, sort, s_date, e_date, RESULT_PATH, keywords, chunk_index, result_list):
    # Calculate the start and end indices for the current chunk
    chunk_size = len(companies) // 20
    start = chunk_index * chunk_size
    end = start + chunk_size if chunk_index < 19 else len(companies)

    # Create a WebDriver instance for the current thread
    driver = webdriver.Chrome(options=options)
    driver.implicitly_wait(10)
    s_date = s_date.replace(".", "")
    e_date = e_date.replace(".", "")
    result, date_text, title_text, source_text, summary_contents_text, contents_text, link_text = initialize_variables()
    df_list = []

    for i in range(start, end):
        company = companies[i]
        page = 1
        while page <= maxpage:
            url = f"https://search.naver.com/search.naver?where=news&query={company}&sort={sort}&ds={s_date}&de={e_date}&nso=so%3Ar%2Cp%3Afrom{s_date}to{e_date}%2Ca%3A&start={page}"
            driver.get(url)
            html = driver.page_source
            result = soup_function(html, date_text, title_text, source_text, contents_text, summary_contents_text,link_text)
            result_df = pd.DataFrame(result)
            if len(result_df) > 0:
                result_df['Company'] = company
                result_df=result_df[result_df['title'].str.contains(company)] #기업명 체크
                df_list.append(result_df)
            page += 10

    # Concatenate DataFrames for the current chunk
    if df_list:
        temp_df = pd.concat(df_list, ignore_index=True)
        joined_keywords='|'.join(keywords)
        temp_df = temp_df[temp_df['contents_full'].str.contains(joined_keywords) & temp_df['contents_summary'].str.contains(joined_keywords)] #키워드 체크
        temp_df['date'] = np.where(temp_df['date'].str.contains('시간 전|분 전|일 전|주 전'), temp_df['date'].apply(extract_hours_and_subtract), temp_df['date']) 
        if len(temp_df) > 0:
            result_list.append(temp_df)
        # output_file_name = f'chunk_{chunk_index}_네이버_기사들.csv'
        # temp_df.to_csv(RESULT_PATH + output_file_name, index=False, encoding='utf-8-sig')

    driver.quit()

def sort_and_save(combined_df, RESULT_PATH):
    print('Saving Result....')
    currentDate = datetime.now() 
    combined_df = combined_df.drop_duplicates(subset=['link'])
    combined_df=combined_df.sort_values(by='date', ascending=False)
    output_file_name = f'{currentDate.strftime("%Y_%m_%d")}_네이버_기사들.csv'
    output = os.path.join(RESULT_PATH,output_file_name)
    combined_df.to_csv(output, index=False, encoding='utf-8-sig')
    print(f'Result saved in: {RESULT_PATH} as file name: {output_file_name}')

def multi_thread_crawling(maxpage, companies, sort, s_date, e_date, keywords, RESULT_PATH):
    threads = []
    result_list = []
    print('Crawling....')
    for chunk_index in range(10):
        thread = threading.Thread(target=crawl_companies_chunk, args=(companies, maxpage, sort, s_date, e_date, RESULT_PATH, keywords, chunk_index, result_list))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()
    print('Crawling Done!!')

    if len(result_list) > 1:
        combined_df = pd.concat(result_list)
        sort_and_save(combined_df, RESULT_PATH)
    elif len(result_list) == 1:
        combined_df = result_list[0]
        sort_and_save(combined_df, RESULT_PATH)
    else:
        print('No Results Available. No Data To Save')

def extract_hours_and_subtract(date_str):
    timestamp = date_str
    if '시간 전' in date_str:
        hours = int(date_str.split('시간 전')[0])
        delta = timedelta(hours=hours)
        timestamp =datetime.now() - delta
        timestamp=timestamp.strftime('%Y.%m.%d')

    elif '분 전' in date_str:
        minutes = int(date_str.split('분 전')[0])
        delta = timedelta(minutes=minutes)
        timestamp =datetime.now() - delta
        timestamp=timestamp.strftime('%Y.%m.%d')

    elif '일 전' in date_str:
        days = int(date_str.split('일 전')[0])
        delta = timedelta(days)
        timestamp =datetime.now() - delta
        timestamp=timestamp.strftime('%Y.%m.%d')

    elif '주 전' in date_str:
        weeks = int(date_str.split('주 전')[0])
        delta = timedelta(weeks=weeks)
        timestamp =datetime.now() - delta
        timestamp=timestamp.strftime('%Y.%m.%d')
    else:
        timestamp = date_str

    return timestamp


def process():
    # info_main = input("="*50+"\n"+"입력 형식에 맞게 입력해주세요."+"\n"+" 시작하시려면 Enter를 눌러주세요."+"\n"+"="*50)

    #엑셀로 저장하기 위한 변수 (사용자가 저장을 다른데 원할 시 변경 가능 Default은 Working Directory)
    WORKING_DIRECTORY = os.getcwd()
    FOLDER_NAME = 'CRAWLING_RESULT'
    companies, keywords = get_basic_info()
    RESULT_PATH = os.path.join(WORKING_DIRECTORY,FOLDER_NAME)
    if not os.path.exists(RESULT_PATH):
        os.mkdir(RESULT_PATH)
    
    maxpage = int(input("최대 크롤링할 페이지 수 입력하시오: "))
    # query = input("검색어 입력: ")
    sort = input("뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): ")    #관련도순=0  최신순=1  오래된순=2
    s_date = input("시작날짜 입력 (####.##.##):")  
    e_date = input("끝날짜 입력:") 
    start = time.time()
    multi_thread_crawling(maxpage, companies, sort, s_date, e_date, keywords, RESULT_PATH)
    end = time.time()
    process_time = round(end-start,0)
    print(f"Process Time is {str(timedelta(seconds=process_time))}")
    # isPath = False
    # while not isPath:
    #     temp_path = input("파일 저정을 위한 Path를 지정해주세요 (지정 안할 시 자동을 해당 파일이 있는 위치에 폴더가 생성되어 저장됩니다.): ").strip()
    #     if os.path.isdir(temp_path):
    #         RESULT_PATH = temp_path
    #         isPath = True
    #     elif temp_path == 'exit':  
    #         print("'exit'이 입력되어 프로그램이 종료됩니다.")
    #         return -1
    #     elif temp_path == '':
    #         isPath = True
    #         continue
    #     else:
    #         temp_path = input("입력된 경로가 폴더가 아니거나 존재하지 않습니다. 다시 확인하고 입력 해주세요. 해당 프로그램 종료 원할 시 'exit'을 입력해주세요: ").strip()

    # maxpage = 1
    # query =''
    # sort = '0'
    # s_date = '2023.11.06'
    # e_date='2023.11.06'
    

if __name__ == '__main__':    # 프로그램의 시작점일 때만 아래 코드 실행
    process()
